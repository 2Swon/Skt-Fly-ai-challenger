{"cells":[{"cell_type":"markdown","metadata":{"id":"250b088e"},"source":["# DQN으로 Atari 게임 강화학습\n","\n","먼저 여러가지 설정 변수 정의"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":14920,"status":"ok","timestamp":1704679030794,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"be000272-b287-41c7-b5ed-1bd352af1a71"},"outputs":[],"source":["import numpy as np\n","import gym\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as T\n","import torch.utils\n","# Configuration paramaters for the whole setup\n","seed = 42\n","gamma = 0.99  # Discount factor for past rewards\n","epsilon = 1.0  # Epsilon greedy parameter\n","epsilon_min = 0.1  # Minimum epsilon greedy parameter\n","epsilon_max = 1.0  # Maximum epsilon greedy parameter\n","epsilon_interval = (\n","    epsilon_max - epsilon_min\n",")  # Rate at which to reduce chance of random action being taken\n","batch_size = 64  # Size of batch taken from replay buffer\n","max_steps_per_episode = 1000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOg2EmpouOgN"},"outputs":[],"source":["!pip install gym[atari,accept-rom-license]==0.26.2"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1704679030795,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"6HvQZxpmuFKn","outputId":"25b0e8e9-92cc-4a1f-a8f5-2fdbca4c9e77"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'0.26.2'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["gym.__version__"]},{"cell_type":"markdown","metadata":{"id":"82a3fe92"},"source":["### Atari 게임 환경\n","\n","Gym env 의 observation\n","- Observation의 모양: (210, 160, 3)\n","\n","액션 정의\n","- 0: NOOP\n","- 1: FIRE\n","- 2: RIGHT\n","- 3: LEFT"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":756,"status":"ok","timestamp":1704679034698,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"5bdb4550-1558-4ba7-a910-9faa34652511"},"outputs":[],"source":["env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode='rgb_array')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21890,"status":"ok","timestamp":1704678115738,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"y-vcDA3ju2dB","outputId":"4343feaa-5563-4754-ac3c-b473b9caed7f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1704679039563,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"FS0yG94qukVl","outputId":"48a01f90-8878-4859-c3a2-2b16dad4c3c6"},"outputs":[{"data":{"text/plain":["array([[[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       ...,\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]]], dtype=uint8)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["obs, info = env.reset()\n","obs"]},{"cell_type":"markdown","metadata":{"id":"5477cab4"},"source":["### 네트워크 정의하기\n","\n","참고: Conv2d 파라미터\n","* in_channels (int) – Number of channels in the input image\n","* out_channels (int) – Number of channels produced by the convolution\n","* kernel_size (int or tuple) – Size of the convolving kernel\n","* stride (int or tuple, optional) – Stride of the convolution. Default: 1\n","* padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\n","* padding_mode (str, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":355,"status":"ok","timestamp":1704679042395,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"f85ef96c-cc48-426a-b2f3-12e002502bc9"},"outputs":[],"source":["num_actions = env.action_space.n\n","\n","class QModel(nn.Module):\n","    def __init__(self, num_actions):\n","        super(QModel, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(3136, 512)\n","        self.fc2 = nn.Linear(512, num_actions)\n","\n","    def forward(self, x):\n","        x = nn.functional.relu(self.conv1(x))\n","        x = nn.functional.relu(self.conv2(x))\n","        x = self.dropout(x)\n","        x = nn.functional.relu(self.conv3(x))\n","        x = self.flatten(x)\n","        x = nn.functional.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        action = self.fc2(x)\n","        return action"]},{"cell_type":"markdown","metadata":{"id":"21a23e4c"},"source":["### 모델 빌딩 \u0026 로스 및 최적화 계산기 만들기"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":437,"status":"ok","timestamp":1704679043946,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f"},"outputs":[],"source":["# The first model makes the predictions for Q-values which are used to\n","# make a action.\n","model = QModel(num_actions)\n","\n","# Build a target model for the prediction of future rewards.\n","# The weights of a target model get updated every 10000 steps thus when the\n","# loss between the Q-values is calculated the target Q-value is stable.\n","model_target = QModel(num_actions)\n","\n","loss_function = nn.SmoothL1Loss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"]},{"cell_type":"markdown","metadata":{"id":"a859f550"},"source":["### Replay Buffer 정의"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":275,"status":"ok","timestamp":1704679044633,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"8eed8ce4-5a53-457f-bc40-fda577b2ec29"},"outputs":[],"source":["# Experience replay buffers\n","action_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","episode_reward_history = []\n","running_reward = 0\n","episode_count = 0\n","frame_count = 0\n","\n","# Number of frames to take random action and observe output\n","epsilon_random_frames = 50000\n","# Number of frames for exploration\n","epsilon_greedy_frames = 100000.0\n","# Maximum replay length\n","# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n","max_memory_length = 500000\n","# Train the model after 4 actions\n","update_after_actions = 4\n","# How often to update the target network\n","update_target_network = 10000"]},{"cell_type":"markdown","metadata":{"id":"eda734a1"},"source":["### 전처리\n","\n","Observation 을 QModel의 입력 타입으로 전처리\n","\n","- PIL Image 객체로 바꾸고\n","- 그레이 스케일로 바꾸고\n","- 84 * 84 짜리 이미지로 리사이징\n","- 지금까지 np.array이니 torch.tensor로 캐스팅\n","- 마지막으로 batch 축 추가"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1704679045773,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"abf51fd4"},"outputs":[],"source":["preprocess = T.Compose([\n","    T.ToPILImage(),\n","    T.Grayscale(),\n","    T.Resize((84, 84)),\n","    T.ToTensor()\n","])\n","\n","# Function to preprocess the state\n","def preprocess_state(state):\n","    state = preprocess(state).unsqueeze(0)\n","    return state"]},{"cell_type":"markdown","metadata":{"id":"61693680"},"source":["### Epsilon-greedy 액션 선택 함수\n","\n","학습시 에피소드 생성하면서 사용 (주의: 입력은 batch axis 없음)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1704679046869,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"f3f9bd3c"},"outputs":[],"source":["# Function to select an action\n","def get_greedy_epsilon(model, state):\n","    global epsilon\n","\n","    #if frame_count \u003c epsilon_random_frames or np.random.rand(1)[0] \u003c epsilon:\n","    if np.random.rand(1)[0] \u003c epsilon:\n","        action = np.random.randint(num_actions)\n","    else:\n","        with torch.no_grad():\n","            # add a batch axis\n","            #state = state.unsqueeze(0)\n","            # compute the q-values\n","            q_values = model(state)\n","            # the action of maximum q-value\n","            action = q_values.argmax().item()\n","\n","    # decay epsilon\n","    epsilon -= epsilon_interval / epsilon_greedy_frames\n","    epsilon = max(epsilon, epsilon_min)\n","\n","    return action"]},{"cell_type":"markdown","metadata":{"id":"ea6728b0"},"source":["### Greedy 액션 선택 함수\n","\n","나중에 evaluation 시 사용"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1704679047356,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"8025c8a5"},"outputs":[],"source":["def get_greedy_action(model, state):\n","    global epsilon\n","\n","    with torch.no_grad():\n","        #state = state.unsqueeze(0) # batch dimension\n","        q_values = model(state)\n","        action = q_values.argmax().item()\n","\n","    return action"]},{"cell_type":"markdown","metadata":{"id":"b99ac400"},"source":["### Update 파트\n","\n","- Replay buffer 에서 batch하나를 샘플링하고,\n","- model을 update한다."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1704679048387,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"e49c6175"},"outputs":[],"source":["# sample a batch of _batch_size from replay buffers\n","# return numpy.ndarrays\n","def sample_batch(_batch_size):\n","    # Get indices of samples for replay buffers\n","    indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n","\n","    state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n","    state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n","    rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n","    action_sample = np.array([action_history[i] for i in indices])\n","    done_sample = np.array([float(done_history[i]) for i in indices])\n","\n","    return state_sample, state_next_sample, rewards_sample, action_sample, done_sample"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1704679049121,"user":{"displayName":"성원이","userId":"15753979272711670953"},"user_tz":-540},"id":"b97e7ba6"},"outputs":[],"source":["# Function to update the Q-network\n","def update_network():\n","    # sample a batch of ...\n","    state_sample, state_next_sample, rewards_sample, action_sample, done_sample = \\\n","        sample_batch(batch_size)\n","\n","    # Convert numpy arrays to PyTorch tensors\n","    state_sample = torch.tensor(state_sample, dtype=torch.float32)\n","    state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32)\n","    action_sample = torch.tensor(action_sample, dtype=torch.int64)\n","    rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32)\n","    done_sample = torch.tensor(done_sample, dtype=torch.float32)\n","\n","    # Compute the target Q-values for the states\n","    with torch.no_grad():\n","        future_rewards = model_target(state_next_sample)\n","\n","        # compute the q-value for the next state and the action maximizing the q-value\n","        max_q_values = future_rewards.max(dim=1).values\n","\n","        # compute the target q-value\n","        # if the step was final, max_q_values should not be added\n","        target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n","\n","    # It's forward propagation! Compute the Q-values for the taken actions\n","    q_values = model(state_sample)\n","    q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n","\n","    # Compute the loss\n","    loss = loss_function(q_values_action, target_q_values)\n","\n","    # Perform the optimization step\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"90ed4dcc-f24e-4990-879a-c2f6af51a063"},"source":["# Run DQN Tranining"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"43c0a265"},"outputs":[{"name":"stdout","output_type":"stream","text":["Episode: 10, Frame count: 7120, Running reward: 1.3\n"]}],"source":["while True:  # Run until solved\n","    state, info = env.reset()\n","    state, reward, done, _, info = env.step(1)\n","    state = preprocess_state(state)\n","    episode_reward = 0\n","\n","    for timestep in range(1, max_steps_per_episode):\n","        frame_count += 1\n","\n","        # Select an action\n","        action = get_greedy_epsilon(model, state)\n","\n","        # Take the selected action\n","        state_next, reward, done, _, info = env.step(action)\n","        state_next = preprocess_state(state_next)\n","\n","        episode_reward += reward\n","\n","        # Store the transition in the replay buffer\n","        action_history.append(action)\n","        state_history.append(state)\n","        state_next_history.append(state_next)\n","        rewards_history.append(reward)\n","        done_history.append(done)\n","\n","        state = state_next\n","\n","        # Update every fourth frame and once batch size is over 32\n","        if frame_count % update_after_actions == 0 and len(done_history) \u003e batch_size:\n","            update_network()\n","\n","        if frame_count % update_target_network == 0:\n","            model_target.load_state_dict(model.state_dict())\n","\n","        # Limit the state and reward history\n","        if len(rewards_history) \u003e max_memory_length:\n","            del rewards_history[:1]\n","            del state_history[:1]\n","            del state_next_history[:1]\n","            del action_history[:1]\n","            del done_history[:1]\n","\n","        if done:\n","            break\n","\n","    episode_count += 1\n","    episode_reward_history.append(episode_reward)\n","\n","    # Update running reward to check condition for solving\n","    if len(episode_reward_history) \u003e 100:\n","        del episode_reward_history[:1]\n","    running_reward = np.mean(episode_reward_history)\n","\n","    if episode_count % 10 == 0:\n","        print(f\"Episode: {episode_count}, Frame count: {frame_count}, Running reward: {running_reward}\")\n","\n","    if episode_count % 5000 == 0:\n","        torch.save(model, 'model.{}'.format(episode_count))\n","    if running_reward \u003e 20:\n","        print(f\"Solved at episode {episode_count}!\")\n","        break\n","    if episode_count % 200 == 0:\n","        break\n","\n","\n","torch.save(model, 'model.final')"]},{"cell_type":"markdown","metadata":{"id":"4984b880-e427-48cb-bf91-13a91d6529f5"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5c198b4e-b0e4-4fd4-821d-be602c2dbc5a"},"outputs":[],"source":["import time, sys\n","from IPython.display import clear_output\n","from matplotlib import animation\n","import matplotlib.pyplot as plt\n","import glob\n","import imageio\n","\n","anim_file = 'atari.gif'\n","\n","turn =  0\n","board, info = env.reset()\n","state = preprocess_state(board)\n","board, reward, done, _, info = env.step(1)\n","state = preprocess_state(board)\n","plt.imshow(board)\n","plt.savefig('image_at_turn_{:04d}.png'.format(turn))\n","\n","for timestep in range(1, 100):\n","    turn += 1\n","    action = get_greedy_action(model, state.to(device))\n","    print(action)\n","    board, reward, done, _, info = env.step(action)\n","    state = preprocess_state(board)\n","    plt.imshow(board)\n","    plt.savefig('image_at_turn_{:04d}.png'.format(turn))\n","\n","    if done:\n","        break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24d7b802"},"outputs":[],"source":["# generate animated gif file\n","with imageio.get_writer(anim_file, mode='I') as writer:\n","    filenames = glob.glob('image_at_turn_*.png')\n","    filenames = sorted(filenames)\n","    for filename in filenames:\n","        print(filename)\n","        image = imageio.imread(filename)\n","        writer.append_data(image)\n","    image = imageio.imread(filename)\n","    writer.append_data(image)"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}